# -*- coding: utf-8 -*-
"""custom-fine-tunning-distilbert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QAeBwg07O6HgH3wyxg5y-es5gWw5ZeVT
"""

!pip install datasets

from datasets import load_dataset

dataset = load_dataset('emotion')

dataset['train']

!pip install transformers

from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(dataset['train']['text'], truncation=True, padding=True)
test_encodings = tokenizer(dataset['test']['text'], truncation=True, padding=True)

import tensorflow as tf

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    dataset['train']['label']
))

test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    dataset['test']['label']
))

from transformers import DistilBertTokenizerFast
from transformers import TFDistilBertForSequenceClassification

model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=6)

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])
model.fit(train_dataset.shuffle(100).batch(16),
          epochs=3,
          batch_size=16,
          validation_data=test_dataset.shuffle(100).batch(16))

